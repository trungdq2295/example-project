DynamoDB  Stream Processing
	Ordered stream of item-level modifications (create/update/delete) in a table
	Data Retention for up to 24 hours
	Streams records can be:
	  Sent to Kinesis Data Streams
	  Read by AWS Lambda
	  Read by Kinesis Client Library applications
	Use cases:
		React to changes in real-time (welcome email to users)
		Real-time usage analytics
		Insert into derivative tables
		Implement cross-region replication
		Invoke AWS Lambda on changes to your DynamoDB table
  Ability to choose the information that will be written to the stream
    KEYS_ONLY - only the key attributes of the modify item
    NEW_IMAGE - the entire item, as it appears after it was modified
    OLD_IMAGE - the entire item, as it appears before it was modified
    NEW_AND_OLD_IMAGES - both the new and the old images of the item
  DynamoDB Streams are made of shards, just like Kinesis Data Streams
  You don't provision shard, this is automated by AWS
  Records are not retroactively populated in a stream after enabling it
    This mean the data was changed before enabling, the stream doesn't record

  DynamoDB Streams & AWS Lambda
    You need to define an Event Source Mapping to read from a DynamoDB Streams
    You need to ensure the Lambda function has the appropriate permissions
    Your Lambda function is invoked synchronously

DynamoDB Streams
	24 hours retention
	Limited # of consumer
	Process using AWS Lambda Trigger, or DynamoDB Stream Kinesis Adapter

Kinesis Data Streams (newer)
	1 year retention
	High # of consumer
	Process using AWS Lambda, Kinesis Data Analytics, Kinesis Data firehose, AWS Glue Streaming ETL,...
	